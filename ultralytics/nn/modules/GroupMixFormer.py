# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F
import math # Import math for sqrt

# 從 timm 導入必要的模塊，如果未安裝，請先安裝 pip install timm
try:
    from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
    from timm.models.layers import DropPath, to_2tuple, trunc_normal_
except ImportError:
    print("Please install timm: pip install timm")
    # Fallback basic implementations if timm is not available
    IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)
    IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)
    class DropPath(nn.Module):
        def __init__(self, drop_prob=None):
            super(DropPath, self).__init__()
            self.drop_prob = drop_prob
        def forward(self, x):
            if self.drop_prob == 0. or not self.training:
                return x
            keep_prob = 1 - self.drop_prob
            shape = (x.shape[0],) + (1,) * (x.ndim - 1)
            random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
            random_tensor.floor_()
            output = x.div(keep_prob) * random_tensor
            return output
    def to_2tuple(x):
        if isinstance(x, tuple):
            return x
        return (x, x)
    # Simplified trunc_normal_ for fallback
    def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
        # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
        def norm_cdf(x):
            return (1. + math.erf(x / math.sqrt(2.))) / 2.
        if (mean < a - 2 * std) or (mean > b + 2 * std):
            print("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                    "The distribution of values may be incorrect.",)
        with torch.no_grad():
            # Values are generated by using a truncated uniform distribution and
            # then using the inverse CDF for the normal distribution.
            # Get upper and lower cdf values
            l = norm_cdf((a - mean) / std)
            u = norm_cdf((b - mean) / std)
            # Uniformly fill tensor with values from [l, u], then translate to
            # [2l-1, 2u-1].
            tensor.uniform_(2 * l - 1, 2 * u - 1)
            # Use inverse cdf transform for normal distribution to get truncated
            # standard normal
            tensor.erfinv_()
            # Transform to proper mean, std
            tensor.mul_(std * math.sqrt(2.))
            tensor.add_(mean)
            # Clamp to ensure it's within the bounds
            tensor.clamp_(min=a, max=b)
            return tensor

# 從 einops 導入必要的模塊，如果未安裝，請先安裝 pip install einops
try:
    from einops import rearrange
except ImportError:
    print("Please install einops: pip install einops")
    # Provide a basic placeholder for rearrange if einops is not installed
    # This won't work for complex patterns but covers the specific ones used here.
    def rearrange(x, pattern, **kwargs):
        print("Warning: einops is not installed. Using basic rearrange.")
        if pattern == 'B h (H W) Ch -> B (h Ch) H W':
             B, h, N, Ch = x.shape
             # H, W must be provided in kwargs
             if 'H' not in kwargs or 'W' not in kwargs:
                 raise ValueError("H and W must be provided for rearrange pattern 'B h (H W) Ch -> B (h Ch) H W' without einops")
             H = kwargs['H']; W = kwargs['W']
             if N != H * W: raise ValueError(f"Rearrange check failed: N({N}) != H({H})*W({W})")
             return x.permute(0, 1, 3, 2).reshape(B, h * Ch, H, W) # B, h, Ch, N -> B, h*Ch, H, W
        elif pattern == 'B (h Ch) H W -> B h (H W) Ch':
            B, hCh, H, W = x.shape
            # h must be provided in kwargs
            if 'h' not in kwargs:
                raise ValueError("h must be provided for rearrange pattern 'B (h Ch) H W -> B h (H W) Ch' without einops")
            h = kwargs['h']
            if hCh % h != 0: raise ValueError(f"Rearrange check failed: hCh({hCh}) not divisible by h({h})")
            Ch = hCh // h
            return x.reshape(B, h, Ch, H*W).permute(0, 1, 3, 2) # B, h, Ch, N -> B, h, N, Ch
        else:
            raise NotImplementedError(f"Unsupported rearrange pattern without einops: {pattern}")


from functools import partial
from torch import nn, einsum

# --- Model Specifications (Ensure dimensions are divisible by seg=4) ---
GroupMixFormerMiny_SPECS = {
    "embedding_dims": [40, 80, 160, 160], # Divisible by 4
    "serial_depths": [3, 3, 12, 4],
    "num_heads": [2, 2, 2, 2], # Example: ensure heads match window split logic
    "mlp_ratios": [4, 4, 4, 4],
}

GroupMixFormerTiny_SPECS = {
    "embedding_dims": [80, 160, 200, 240], # Divisible by 4
    "serial_depths": [4, 4, 12, 4],
    "num_heads": [4, 4, 4, 4], # Example: ensure heads match window split logic
    "mlp_ratios": [4, 4, 4, 4],
}

GroupMixFormerSmall_SPECS = {
    "embedding_dims": [80, 160, 320, 320], # Divisible by 4 and 5
    "serial_depths": [2, 4, 12, 4],
    "num_heads": [4, 4, 4, 4], # Example, needs check (e.g., stage 0 dim 80 / seg 4 = 20, att_dim 60. heads=2 ok)
    "mlp_ratios": [4, 4, 4, 4],
}

GroupMixFormerBase_SPECS = {
    "embedding_dims": [200, 240, 320, 480], # Divisible by 4 (and 3, 6, 8, 12...)
    "serial_depths": [8, 8, 12, 8],
    "num_heads": [6, 6, 6, 6], # Example, needs check (e.g., stage 0 dim 96 / seg 4 = 24, att_dim 72. heads=3 ok)
    "mlp_ratios": [2, 2, 4, 4],
}

GroupMixFormerLarge_SPECS = {
    "embedding_dims": [240, 320, 360, 480], # Divisible by 4 (and 3, 6, 8, 12...)
    "serial_depths": [8, 10, 30, 10],
    "num_heads": [6, 6, 6, 6], # Example, needs check (e.g., stage 0 dim 96 / seg 4 = 24, att_dim 72. heads=3 ok)
    "mlp_ratios": [4, 4, 2, 2],
}

MODEL_SPECS = {
    "GroupMixFormerMiny": GroupMixFormerMiny_SPECS,
    "GroupMixFormerTiny": GroupMixFormerTiny_SPECS,
    "GroupMixFormerSmall": GroupMixFormerSmall_SPECS,
    "GroupMixFormerBase": GroupMixFormerBase_SPECS,
    "GroupMixFormerLarge": GroupMixFormerLarge_SPECS,
}

__all__ = list(MODEL_SPECS.keys()) + ['GroupMixFormer']

# --- Helper Modules ---
class Mlp(nn.Module):
    """ Feed-forward network (FFN, a.k.a. MLP) class. """
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x); x = self.act(x); x = self.drop(x)
        x = self.fc2(x); x = self.drop(x)
        return x

class SeparableConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):
        super(SeparableConv2d, self).__init__()
        # Depthwise convolution
        self.depthwise_conv = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)
        # Pointwise convolution
        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)

    def forward(self, x):
        x = self.depthwise_conv(x) # Apply depthwise conv first
        x = self.pointwise_conv(x) # Then apply pointwise conv
        return x

class Agg_Local(nn.Module):
    def __init__(self, seg_dim):
        super().__init__()
        self.conv = SeparableConv2d(seg_dim * 3, seg_dim, 3, 1, 1)
        self.norm = nn.BatchNorm2d(seg_dim)
        self.act = nn.Hardswish()

    def forward(self, x):
        # Input x shape: [B, 3*seg_dim, H, W]
        x = self.conv(x) # -> [B, seg_dim, H, W]
        x = self.norm(x); x = self.act(x)
        b, c, h, w = x.shape
        x = x.reshape(b, c, h * w).permute(0, 2, 1) # [B, N, seg_dim] (N=H*W)
        return x

class Aggregator(nn.Module):
    def __init__(self, dim, seg=4): # Changed default seg to 4
        super().__init__()
        self.dim = dim; self.seg = seg
        if dim % seg != 0: raise ValueError(f"Dim {dim} not divisible by seg {seg}")
        seg_dim = self.dim // self.seg

        self.norm0 = nn.BatchNorm2d(seg_dim); self.act0 = nn.Hardswish()
        self.agg1 = SeparableConv2d(seg_dim, seg_dim, 3, 1, 1)
        self.norm1 = nn.BatchNorm2d(seg_dim); self.act1 = nn.Hardswish()
        self.agg2 = SeparableConv2d(seg_dim, seg_dim, 5, 1, 2) # Kernel 5, Padding 2 for 'same'
        self.norm2 = nn.BatchNorm2d(seg_dim); self.act2 = nn.Hardswish()
        self.agg_local_processor = Agg_Local(seg_dim)

    def forward(self, x, size, num_head):
        # Input x shape: [3*B, N, C], size: tuple (H, W) corresponding to N
        _3B, N, C = x.shape
        H_in, W_in = size # H, W passed from previous layer, should match N
        B = _3B // 3

        # --- Verify H, W from N ---
        # Use the passed size directly as it should be correct from PatchEmbed
        if H_in == 0 or W_in == 0 or H_in * W_in != N:
            # This case should ideally not happen if size is passed correctly
            print(f"Warning: Aggregator N={N} mismatch with size={size}. Re-inferring H, W.")
            H = W = int(math.sqrt(N))
            if H * W != N:
                # Fallback if not square and size is invalid/mismatched
                 raise ValueError(f"Cannot determine H, W for N={N} with invalid size={size}")
            print(f"  Aggregator using inferred H={H}, W={W}.")
        else:
            H, W = H_in, W_in # Use provided size
        # --- End Verify H, W ---

        # Reshape using verified H, W
        try:
             # Reshape: [3*B, N, C] -> [3*B, C, N] -> [3*B, C, H, W]
             x_spatial = x.transpose(1, 2).view(_3B, C, H, W)
        except RuntimeError as e:
             print(f"RuntimeError in Aggregator reshape: Input shape={x.shape}, N={N}, C={C}, Target H={H}, W={W}")
             raise e

        seg_dim = self.dim // self.seg
        # Split along the channel dimension (dim=1)
        x_splits = x_spatial.split([seg_dim]*self.seg, dim=1)

        # Local Path
        local_qkv_split = x_splits[3] # Shape: [3*B, seg_dim, H, W]
        # Reshape to combine the 3 parts (q, k, v) for local processing
        # [3, B, seg_dim, H, W] -> [B, 3, seg_dim, H, W] -> [B, 3*seg_dim, H, W]
        local_qkv_combined = local_qkv_split.reshape(3, B, seg_dim, H, W).permute(1, 0, 2, 3, 4).reshape(B, 3 * seg_dim, H, W)
        x_local = self.agg_local_processor(local_qkv_combined) # Output shape: [B, N, seg_dim]

        # Attention Path
        x0 = self.act0(self.norm0(x_splits[0])) # Shape: [3*B, seg_dim, H, W]
        x1 = self.act1(self.norm1(self.agg1(x_splits[1]))) # Shape: [3*B, seg_dim, H, W]
        x2 = self.act2(self.norm2(self.agg2(x_splits[2]))) # Shape: [3*B, seg_dim, H, W]
        # Concatenate along the channel dimension
        x_main = torch.cat([x0, x1, x2], dim=1) # Shape: [3*B, 3*seg_dim, H, W]
        main_C = C // self.seg * (self.seg - 1) # = 3 * seg_dim

        if main_C % num_head != 0: raise ValueError(f"Att dim {main_C} not divisible by heads {num_head}")
        head_dim = main_C // num_head

        # Reshape for attention: [3*B, main_C, H, W] -> [3*B, main_C, N] -> [3, B, main_C, N] -> [3, B, h, Ch, N] -> [3, B, h, N, Ch]
        x_main = x_main.flatten(2) # Shape: [3*B, main_C, N]
        x_main = x_main.reshape(3, B, num_head, head_dim, N).permute(0, 1, 2, 4, 3) # Shape: [3, B, h, N, Ch]

        return x_main, x_local


class ConvRelPosEnc(nn.Module):
    """ Convolutional relative position encoding. """
    def __init__(self, Ch, h, window):
        super().__init__()
        if not isinstance(window, dict): raise ValueError("Window must be a dict")
        window_heads_sum = sum(v for v in window.values() if isinstance(v, int) and v > 0)
        if window_heads_sum != h: raise ValueError(f"Window heads sum {window_heads_sum} != total heads {h}")
        self.window = window; self.h = h; self.Ch = Ch
        self.conv_list = nn.ModuleList()
        self.head_splits = []; self.channel_splits = []
        # Ensure consistent order for splitting later
        sorted_window_items = sorted(window.items())
        for cur_window, cur_head_split in sorted_window_items:
            if cur_head_split == 0: continue
            dilation = 1; padding_size = (cur_window + (cur_window - 1) * (dilation - 1)) // 2
            cur_conv = nn.Conv2d(cur_head_split * Ch, cur_head_split * Ch,
                                 kernel_size=(cur_window, cur_window), padding=(padding_size, padding_size),
                                 dilation=(dilation, dilation), groups=cur_head_split * Ch, bias=True) # Added bias=True, common for pos enc
            self.conv_list.append(cur_conv)
            self.head_splits.append(cur_head_split)
            self.channel_splits.append(cur_head_split * Ch)

    def forward(self, q, v, size):
        # q shape: [B, h, N, Ch], v shape: [B, h, N, Ch], size: tuple (H, W) corresponding to N
        B, h_in, N, Ch_in = q.shape
        assert h_in == self.h and Ch_in == self.Ch, f"Input shape mismatch: q={q.shape}, expected h={self.h}, Ch={self.Ch}"
        H_in, W_in = size

        # --- Verify H, W from N ---
        if H_in == 0 or W_in == 0 or H_in * W_in != N:
            print(f"Warning: ConvRelPosEnc N={N} mismatch with size={size}. Re-inferring H, W.")
            H = W = int(math.sqrt(N))
            if H * W != N:
                 raise ValueError(f"Cannot determine H, W for N={N} with invalid size={size}")
            print(f"  ConvRelPosEnc using inferred H={H}, W={W}.")
        else:
            H, W = H_in, W_in
        # --- End Verify H, W ---

        # Reshape V using verified H, W for convolution
        try:
            # v: [B, h, N, Ch] -> [B, h, H, W, Ch] -> [B, h*Ch, H, W]
            v_img = rearrange(v, 'B h (H W) Ch -> B (h Ch) H W', H=H, W=W)
        except ValueError as e: # Catch rearrange check failure
            print(f"ValueError in ConvRelPosEnc rearrange (v -> v_img): Input shape={v.shape}, N={N}, inferred H={H}, W={W}. Error: {e}")
            raise e
        except RuntimeError as e:
             print(f"RuntimeError in ConvRelPosEnc rearrange (v -> v_img): Input shape={v.shape}, N={N}, H={H}, W={W}")
             raise e

        # Split v_img according to head counts for different window sizes
        # Ensure split sizes match the number of channels derived from head splits
        if sum(self.channel_splits) != v_img.shape[1]:
             raise ValueError(f"Channel split sum {sum(self.channel_splits)} != v_img channels {v_img.shape[1]}")

        v_img_list = torch.split(v_img, self.channel_splits, dim=1)
        conv_v_img_list = [conv(x) for conv, x in zip(self.conv_list, v_img_list)]
        conv_v_img = torch.cat(conv_v_img_list, dim=1) # Shape: [B, h*Ch, H, W]

        # Reshape back to match q's shape using verified H, W
        try:
            # conv_v_img: [B, h*Ch, H, W] -> [B, h, Ch, H*W] -> [B, h, N, Ch]
            conv_v = rearrange(conv_v_img, 'B (h Ch) H W -> B h (H W) Ch', h=h_in)
        except ValueError as e: # Catch rearrange check failure
            print(f"ValueError in ConvRelPosEnc rearrange (conv_v_img -> conv_v): Input shape={conv_v_img.shape}, target h={h_in}. Error: {e}")
            raise e
        except RuntimeError as e:
             print(f"RuntimeError in ConvRelPosEnc rearrange (conv_v_img -> conv_v): Input shape={conv_v_img.shape}, H={H}, W={W}")
             raise e

        # Element-wise multiply q with the positionally encoded v
        EV_hat_img = q * conv_v
        return EV_hat_img


class EfficientAtt(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., seg=4): # Added seg here
        super().__init__()
        self.dim = dim; self.num_heads = num_heads; self.seg = seg # Use passed seg
        if dim % self.seg != 0: raise ValueError(f"Dim {dim} not divisible by seg {self.seg}")
        seg_dim = dim // self.seg
        self.att_dim = seg_dim * (self.seg - 1); self.local_dim = seg_dim
        if self.att_dim % num_heads != 0: raise ValueError(f"Att dim {self.att_dim} not divisible by heads {num_heads}")
        head_dim = self.att_dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim); self.proj_drop = nn.Dropout(proj_drop)
        self.aggregator = Aggregator(dim=dim, seg=self.seg)

        # Dynamic CRPE Window Definition
        crpe_ch = self.att_dim // num_heads # Channel dim per head for attention part
        window_sizes = [3, 5, 7]; window_splits = {}; heads_remaining = num_heads
        # Distribute heads as evenly as possible among window sizes
        base_split = heads_remaining // len(window_sizes)
        remainder = heads_remaining % len(window_sizes)
        split_counts = [base_split] * len(window_sizes)
        for i in range(remainder): split_counts[i] += 1

        # Assign counts to window sizes
        for i, size in enumerate(window_sizes): window_splits[size] = split_counts[i]

        # Filter out sizes with zero heads assigned
        window_splits = {k: v for k, v in window_splits.items() if v > 0}
        assert sum(window_splits.values()) == num_heads, f"Window split sum {sum(window_splits.values())} != num_heads {num_heads}"

        self.crpe = ConvRelPosEnc(Ch=crpe_ch, h=num_heads, window=window_splits)

    def forward(self, x, size):
        # Input x: [B, N, C], size: (H, W) corresponding to N
        B, N, C = x.shape; assert C == self.dim, f"Input channel {C} != Module dim {self.dim}"
        # [B, N, C] -> [B, N, 3*C] -> [B, N, 3, C] -> [3, B, N, C] -> [3*B, N, C]
        qkv = self.qkv(x).reshape(B, N, 3, C).permute(2, 0, 1, 3).reshape(3 * B, N, C)

        # Aggregator processes qkv, returns attention part and local part
        # qkv_att: [3, B, h, N, Ch_att], x_local: [B, N, C_local]
        # size (H, W) is passed to aggregator and CRPE for spatial awareness
        qkv_att, x_local = self.aggregator(qkv, size, self.num_heads)
        q, k, v = qkv_att[0], qkv_att[1], qkv_att[2] # Each [B, h, N, Ch_att]

        # Calculate scaled dot-product attention efficiently
        k_softmax = k.softmax(dim=-1) # Softmax over sequence length N [B, h, N, Ch_att] -> This seems wrong, should be dim=2? Check einsum below
        # Let's re-check the einsum logic from original paper or reference if possible.
        # Assuming standard attention logic applied efficiently:
        # Attention Score: Q * K^T -> einsum('b h n k, b h m k -> b h n m', q, k) -> [B, h, N, N]
        # Apply Softmax: attn_score_softmax = attn_score.softmax(dim=-1)
        # Multiply by V: einsum('b h n m, b h m v -> b h n v', attn_score_softmax, v) -> [B, h, N, Ch_att]

        # The provided einsum seems like a linear attention approximation:
        # 1. k_softmax = k.softmax(dim=2) # Softmax over N [B, h, N, Ch_att] - Normalizes features per token?
        # 2. k_softmax_T_dot_v = einsum('b h n k, b h n v -> b h k v', k_softmax, v) # [B, h, Ch_att, Ch_att] - Weighted sum of V based on K features
        # 3. eff_att = einsum('b h n k, b h k v -> b h n v', q, k_softmax_T_dot_v) # [B, h, N, Ch_att] - Final attn output using Q

        # Let's stick to the provided einsum for now, assuming it's the intended efficient mechanism
        k_softmax = k.softmax(dim=2) # Softmax over N dimension [B, h, N, Ch_att]
        k_softmax = self.attn_drop(k_softmax) # Apply dropout after softmax
        # Calculate the context vector (weighted sum of values) based on normalized keys
        k_softmax_T_dot_v = einsum('b h n k, b h n v -> b h k v', k_softmax, v) # [B, h, Ch_att, Ch_att] - Check dims carefully! k=Ch_att, v=Ch_att
        # Calculate the final attention output by querying the context vector
        eff_att = einsum('b h n k, b h k v -> b h n v', q, k_softmax_T_dot_v) # [B, h, N, Ch_att]

        # Calculate Convolutional Relative Position Encoding
        crpe = self.crpe(q, v, size=size) # Output shape: [B, h, N, Ch_att]

        # Combine efficient attention and CRPE
        x_att = self.scale * eff_att + crpe # Shape: [B, h, N, Ch_att]
        # Reshape attention output back: [B, h, N, Ch_att] -> [B, N, h, Ch_att] -> [B, N, h*Ch_att = att_dim]
        x_att = x_att.transpose(1, 2).reshape(B, N, self.att_dim)

        # Concatenate attention part and local part
        x_combined = torch.cat([x_att, x_local], dim=-1) # Shape: [B, N, att_dim + local_dim = dim]
        assert x_combined.shape == (B, N, self.dim), f"Combine shape mismatch: Got {x_combined.shape}, Expected {(B, N, self.dim)}"

        # Final projection and dropout
        x_out = self.proj(x_combined); x_out = self.proj_drop(x_out) # Shape: [B, N, dim]
        return x_out


class ConvPosEnc(nn.Module):
    """ Convolutional Position Encoding """
    def __init__(self, dim, k=3):
        super(ConvPosEnc, self).__init__()
        # Depthwise convolution
        self.proj = nn.Conv2d(dim, dim, kernel_size=k, stride=1, padding=k // 2, groups=dim, bias=True) # Added bias

    def forward(self, x, size):
        # Input x: [B, N, C], size: tuple (H, W) corresponding to N
        B, N, C = x.shape
        H_in, W_in = size

        # --- Verify H, W from N ---
        if H_in == 0 or W_in == 0 or H_in * W_in != N:
            print(f"Warning: ConvPosEnc N={N} mismatch with size={size}. Re-inferring H, W.")
            H = W = int(math.sqrt(N))
            if H * W != N:
                 raise ValueError(f"Cannot determine H, W for N={N} with invalid size={size}")
            print(f"  ConvPosEnc using inferred H={H}, W={W}.")
        else:
            H, W = H_in, W_in
        # --- End Verify H, W ---

        # Reshape using verified H, W: [B, N, C] -> [B, C, N] -> [B, C, H, W]
        try:
            feat = x.transpose(1, 2).view(B, C, H, W)
        except RuntimeError as e:
             print(f"RuntimeError in ConvPosEnc reshape: Input shape={x.shape}, N={N}, C={C}, Target H={H}, W={W}")
             raise e

        # Apply depthwise convolution for positional encoding
        x_proj = self.proj(feat) # Output shape [B, C, H, W]

        # Add residual connection
        x_out = feat + x_proj

        # Reshape back to [B, N, C]: [B, C, H, W] -> [B, C, N] -> [B, N, C]
        x_out = x_out.flatten(2).transpose(1, 2)
        return x_out

class ConvStem(nn.Module):
    """ Convolutional Stem """
    def __init__(self, in_dim=3, embedding_dims=64):
        super().__init__()
        mid_dim = embedding_dims // 2
        # Conv1: 3 -> mid_dim, Stride 2 (Downsamples by 2)
        self.proj1 = nn.Conv2d(in_dim, mid_dim, kernel_size=3, stride=2, padding=1)
        self.norm1 = nn.BatchNorm2d(mid_dim); self.act1 = nn.Hardswish()
        # Conv2: mid_dim -> embedding_dims, Stride 2 (Downsamples by 2)
        self.proj2 = nn.Conv2d(mid_dim, embedding_dims, kernel_size=3, stride=2, padding=1)
        self.norm2 = nn.BatchNorm2d(embedding_dims); self.act2 = nn.Hardswish()

    def forward(self, x):
        # Input x: [B, C_in, H, W]
        x = self.act1(self.norm1(self.proj1(x))) # [B, mid_dim, H/2, W/2]
        x = self.act2(self.norm2(self.proj2(x))) # [B, embedding_dims, H/4, W/4]
        return x

class PatchEmbedLayer(nn.Module):
    """ Patch Embedding Layer using Separable Convolution """
    def __init__(self, patch_size=2, in_dim=64, embedding_dims=128):
        super().__init__()
        patch_size = to_2tuple(patch_size)
        self.patch_size = patch_size
        # Use SeparableConv2d for patch embedding and downsampling
        # Kernel=3, Padding=1, Stride=patch_size
        self.proj = SeparableConv2d(in_dim, embedding_dims, kernel_size=3, stride=patch_size, padding=1)
        self.norm = nn.BatchNorm2d(embedding_dims); self.act = nn.Hardswish()

    def forward(self, x):
        # Input x: [B, C_in, H_in, W_in] (Output from previous stage or stem)
        B, C_in, H_in, W_in = x.shape

        # Apply projection (SeparableConv + Norm + Act)
        x = self.act(self.norm(self.proj(x))) # [B, C_out, H_out, W_out]
        B_out, C_out, H_out, W_out = x.shape

        # Flatten spatial dimensions: [B, C_out, H_out, W_out] -> [B, C_out, N] -> [B, N, C_out]
        x = x.flatten(2).transpose(1, 2) # Shape: [B, N, C_out] where N = H_out * W_out

        # Return the sequence tensor and its corresponding spatial size (H_out, W_out)
        return x, (H_out, W_out)


class GMA_Block(nn.Module):
    """ Group Mix Aggregation Block """
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path_rate=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, seg=4): # Added seg
        super().__init__()
        self.cpe = ConvPosEnc(dim=dim, k=3) # Convolutional Positional Encoding
        self.norm1 = norm_layer(dim)
        self.att = EfficientAtt(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,
                                attn_drop=attn_drop, proj_drop=drop, seg=seg) # Pass seg to EfficientAtt
        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x_input, size):
        # Input x_input: [B, N, C], size: tuple (H, W) corresponding to N

        # Apply Convolutional Position Encoding with residual connection
        x_cpe = self.cpe(x_input, size) # Output: [B, N, C]

        # First residual path: x_cpe + DropPath(Attention(Norm1(x_cpe)))
        cur = self.norm1(x_cpe)        # Apply LayerNorm
        cur = self.att(cur, size)      # Apply Efficient Attention (needs size)
        x = x_cpe + self.drop_path(cur) # Apply DropPath and add residual

        # Second residual path: x + DropPath(MLP(Norm2(x)))
        cur = self.norm2(x)            # Apply LayerNorm
        cur = self.mlp(cur)            # Apply MLP
        x = x + self.drop_path(cur)    # Apply DropPath and add residual

        return x # Output: [B, N, C]

class GMA_Stage(nn.Module):
    """ Group Mix Aggregation Stage (sequence of GMA_Blocks) """
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path_rate_list=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm, serial_depth=None, seg=4): # Added seg
        super().__init__()
        self.serial_depth = serial_depth
        if drop_path_rate_list is None or len(drop_path_rate_list) != serial_depth:
            raise ValueError(f"drop_path_rate_list length ({len(drop_path_rate_list) if drop_path_rate_list else 0}) != serial_depth ({serial_depth})")

        # Handle single int or list for num_heads and mlp_ratio
        if not isinstance(num_heads, list): num_heads = [num_heads] * serial_depth
        if not isinstance(mlp_ratio, list): mlp_ratio = [mlp_ratio] * serial_depth
        # Ensure lists have correct length if provided shorter
        if len(num_heads) < serial_depth: num_heads = num_heads + [num_heads[-1]] * (serial_depth - len(num_heads))
        if len(mlp_ratio) < serial_depth: mlp_ratio = mlp_ratio + [mlp_ratio[-1]] * (serial_depth - len(mlp_ratio))

        self.gma_stage = nn.ModuleList([
            GMA_Block(
                dim=dim, num_heads=num_heads[i], mlp_ratio=mlp_ratio[i],
                qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, attn_drop=attn_drop,
                drop_path_rate=drop_path_rate_list[i], act_layer=act_layer, norm_layer=norm_layer, seg=seg # Pass seg
            ) for i in range(serial_depth)]
        )

    def forward(self, x, size):
        # Input x: [B, N, C], size: tuple (H, W) corresponding to N
        # Pass input through all blocks in the stage sequentially
        for i in range(self.serial_depth):
            x = self.gma_stage[i](x, size) # Pass size down to each block
        return x # Output: [B, N, C]

def stochastic_depth(drop_path_rate, serial_depths, num_stages):
    """ Creates list of drop path rates for each block in each stage """
    total_depth = sum(serial_depths)
    if total_depth == 0: return [[0.0] * d for d in serial_depths]
    # Linear decay from 0 to drop_path_rate across all blocks
    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]
    dpr_per_stage = []; cur = 0
    # Distribute drop path rates to stages based on their depth
    for depth in serial_depths:
        dpr_per_stage.append(dpr[cur : cur + depth])
        cur += depth
    return dpr_per_stage


# --- Main Model Class ---
class GroupMixFormer(nn.Module):
    """
    GroupMixFormer Vision Transformer Backbone.

    Args:
        model_name (str): Name of the model variant (e.g., "GroupMixFormerTiny").
        in_chans (int): Number of input image channels. Default: 3.
        drop_rate (float): Dropout rate. Default: 0.0.
        attn_drop_rate (float): Attention dropout rate. Default: 0.0.
        drop_path_rate (float): Stochastic depth rate. Default: 0.1.
        norm_layer (nn.Module): Normalization layer. Default: partial(nn.LayerNorm, eps=1e-6).
        act_layer (nn.Module): Activation layer. Default: nn.GELU.
        seg (int): Number of segments for channel splitting in Aggregator/EfficientAtt. Default: 4.
    """
    def __init__(
            self, model_name: str, in_chans: int = 3, drop_rate: float = 0.0,
            attn_drop_rate: float = 0.0, drop_path_rate: float = 0.1,
            norm_layer=partial(nn.LayerNorm, eps=1e-6), act_layer=nn.GELU, seg: int = 4, # Added seg parameter
        ):
        super().__init__()
        if model_name not in MODEL_SPECS: raise ValueError(f"Unknown model_name: {model_name}")
        self.model_name = model_name
        specs = MODEL_SPECS[model_name]
        embedding_dims = specs['embedding_dims'] # List of dims per stage
        serial_depths = specs['serial_depths']   # List of blocks per stage
        num_heads = specs['num_heads']           # List of heads per stage
        mlp_ratios = specs['mlp_ratios']         # List of mlp ratios per stage
        self.num_stages = len(embedding_dims)
        self.seg = seg # Store seg value

        # --- Input Checks ---
        if not isinstance(embedding_dims, list) or len(embedding_dims) != self.num_stages:
            raise ValueError("embedding_dims must be a list of length num_stages")
        if not isinstance(serial_depths, list) or len(serial_depths) != self.num_stages:
            raise ValueError("serial_depths must be a list of length num_stages")
        if not isinstance(num_heads, list) or len(num_heads) != self.num_stages:
             # Allow single int for heads/mlp_ratio, but convert to list internally if needed by stages
             if isinstance(num_heads, int): num_heads = [num_heads] * self.num_stages
             else: raise ValueError("num_heads must be an int or a list of length num_stages")
        if not isinstance(mlp_ratios, list) or len(mlp_ratios) != self.num_stages:
            if isinstance(mlp_ratios, int) or isinstance(mlp_ratios, float): mlp_ratios = [mlp_ratios] * self.num_stages
            else: raise ValueError("mlp_ratios must be an int/float or a list of length num_stages")

        # Check divisibility requirements for seg early
        for i, dim in enumerate(embedding_dims):
             if dim % seg != 0:
                 raise ValueError(f"Stage {i} embedding_dim {dim} is not divisible by seg {seg}")
             att_dim = (dim // seg) * (seg - 1)
             stage_heads = num_heads[i]
             if att_dim % stage_heads != 0:
                 raise ValueError(f"Stage {i} attention dim {att_dim} is not divisible by num_heads {stage_heads}")


        # --- Layer Construction ---
        # Stem: Downsamples input by 4x, sets initial channel dim
        self.conv_stem = ConvStem(in_dim=in_chans, embedding_dims=embedding_dims[0])
        # Current spatial downsampling factor relative to input (Stem = 4x)
        current_stride = 4

        self.patch_embed_layers = nn.ModuleList()
        self.groupmixformer_backbone = nn.ModuleList()
        # Calculate stochastic depth rates per block for all stages
        dpr_per_stage = stochastic_depth(drop_path_rate, serial_depths, self.num_stages)

        # Input dimension for the first PatchEmbed layer is the output of the stem
        in_dim_i = embedding_dims[0]

        for i in range(self.num_stages):
            # Patch Embedding: Downsamples by 2x (patch_size=2), adjusts channels
            # Takes input dim from previous stage (or stem for i=0)
            patch_embed = PatchEmbedLayer(patch_size=2, in_dim=in_dim_i, embedding_dims=embedding_dims[i])
            self.patch_embed_layers.append(patch_embed)
            current_stride *= 2 # Update stride after patch embedding

            # GMA Stage: Sequence of GMA blocks operating at the current resolution/dimension
            stage_heads = num_heads[i]
            stage_mlp_ratio = mlp_ratios[i]
            if not isinstance(stage_heads, int) or stage_heads <= 0:
                raise ValueError(f"Invalid number of heads for stage {i}: {stage_heads}")

            stage = GMA_Stage(
                dim=embedding_dims[i], num_heads=stage_heads, mlp_ratio=stage_mlp_ratio,
                qkv_bias=True, qk_scale=None, drop=drop_rate, attn_drop=attn_drop_rate,
                drop_path_rate_list=dpr_per_stage[i], norm_layer=norm_layer,
                act_layer=act_layer, serial_depth=serial_depths[i], seg=self.seg # Pass seg to stage
            )
            self.groupmixformer_backbone.append(stage)

            # Update input dimension for the *next* stage's PatchEmbed layer
            in_dim_i = embedding_dims[i]

        # Initialize weights
        self.apply(self._init_weights)
        self.width_list = [i.size(1) for i in self.forward(torch.randn(1, 3, 640, 640))] # To store output feature widths if needed

    def _init_weights(self, m):
        """ Initializes weights of the model """
        if isinstance(m, nn.Linear):
            # Use truncated normal initialization for linear layers
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):
            # Initialize normalization layers
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
             # Initialize convolutional layers using Kaiming normal
             nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
             if m.bias is not None:
                 nn.init.constant_(m.bias, 0)

    @torch.jit.ignore
    def no_weight_decay(self):
        """ Returns a set of parameter names that should not be weight decayed """
        no_decay = set()
        # Exclude biases and normalization parameters from weight decay
        for name, param in self.named_parameters():
             if 'norm' in name or 'bias' in name: # Check for 'norm' (LayerNorm, BatchNorm) or 'bias' in name
                 no_decay.add(name)
             # Optionally exclude CRPE conv weights/biases if desired
             # if 'crpe.conv_list' in name:
             #    no_decay.add(name)
        return no_decay

    def forward_features(self, x):
        """ Forward pass through the backbone stages """
        outputs = []
        # Input x: [B, C_in, H, W]
        x = self.conv_stem(x) # [B, D0, H/4, W/4]

        # Iterate through each stage
        for i in range(self.num_stages):
            # Apply Patch Embedding: x becomes [B, N_i, D_i], size is (H_i, W_i)
            # where H_i = H/(4*2^(i+1)), W_i = W/(4*2^(i+1)), N_i = H_i * W_i
            x_patch, size = self.patch_embed_layers[i](x)
            H, W = size # Spatial dimensions corresponding to N_i

            # Apply GMA Stage: Operates on sequence [B, N_i, D_i], needs size (H, W)
            x_gma = self.groupmixformer_backbone[i](x_patch, size) # Output: [B, N_i, D_i]

            B_out, N_out, C_out = x_gma.shape
            # N_out should still be H * W after the GMA stage
            if N_out != H * W:
                 # This should not happen if GMA blocks preserve N
                 print(f"Warning: N mismatch after GMA Stage {i}. Expected N={H*W}, Got N={N_out}. Reshaping might fail.")
                 # Attempt to proceed with H, W from patch embed if reshape below fails

            # Reshape back to 4D tensor for next PatchEmbed layer or final output
            # [B, N_i, D_i] -> [B, H_i, W_i, D_i] -> [B, D_i, H_i, W_i]
            try:
                x = x_gma.reshape(B_out, H, W, C_out).permute(0, 3, 1, 2).contiguous()
            except RuntimeError as e:
                 print(f"RuntimeError in forward_features reshape stage {i}: Input shape={x_gma.shape}, N_out={N_out}, Target H={H}, W={W}, C={C_out}")
                 raise e
            # Append the 4D output of the stage
            outputs.append(x)

        # Return list of feature maps from each stage
        return outputs

    def forward(self, x):
        """ Main forward pass """
        features = self.forward_features(x)
        # Store feature map channel dimensions if not already done
        # if self.width_list is None:
        #      self.width_list = [f.shape[1] for f in features] # Get channel dim (dim=1)
        return features # Return list of [B, C_i, H_i, W_i] tensors

# --- Instantiation Functions ---
# Note: These functions now accept **kwargs which will be passed to GroupMixFormer.__init__
# This allows overriding defaults like in_chans, drop_rate, seg etc.
def GroupMixFormerMiny(**kwargs):
    """ Instantiates GroupMixFormerMiny variant """
    # Set default drop_path_rate if not provided in kwargs
    kwargs.setdefault('drop_path_rate', 0.1)
    model = GroupMixFormer(model_name="GroupMixFormerMiny", **kwargs)
    return model

def GroupMixFormerTiny(**kwargs):
    """ Instantiates GroupMixFormerTiny variant """
    # Set default drop_path_rate if not provided in kwargs
    kwargs.setdefault('drop_path_rate', 0.1)
    model = GroupMixFormer(model_name="GroupMixFormerTiny", **kwargs)
    return model

def GroupMixFormerSmall(**kwargs):
    """ Instantiates GroupMixFormerSmall variant """
    kwargs.setdefault('drop_path_rate', 0.2)
    model = GroupMixFormer(model_name="GroupMixFormerSmall", **kwargs)
    return model

def GroupMixFormerBase(**kwargs):
    """ Instantiates GroupMixFormerBase variant """
    kwargs.setdefault('drop_path_rate', 0.5)
    model = GroupMixFormer(model_name="GroupMixFormerBase", **kwargs)
    return model

def GroupMixFormerLarge(**kwargs):
    """ Instantiates GroupMixFormerLarge variant """
    kwargs.setdefault('drop_path_rate', 0.1)
    model = GroupMixFormer(model_name="GroupMixFormerLarge", **kwargs)
    return model

__all__ = ['GroupMixFormerMiny', 'GroupMixFormerTiny', 'GroupMixFormerSmall', 'GroupMixFormerBase', 'GroupMixFormerLarge', 'GroupMixFormer'] # Export functions and class

# --- Example Usage (Standalone) ---
if __name__ == "__main__":
    device = 'cuda' if torch.cuda.is_available() else 'cpu'; print(f"Using device: {device}")

    # Choose a model variant
    model = GroupMixFormerTiny().to(device)
    # model = GroupMixFormerSmall().to(device)
    # model = GroupMixFormerBase(seg=4).to(device) # Example: explicitly set seg=4 for Base model
    # model = GroupMixFormerBase(seg=6).to(device) # Example: try seg=6 for Base model (dims must be divisible)

    model.eval()
    input_size = (2, 3, 224, 224); # Example Batch size B=2
    inputs = torch.randn(*input_size).to(device)

    print(f"\n--- Testing Model: {model.model_name} (seg={model.seg}) ---")
    print(f"Input shape: {inputs.shape}")

    try:
        with torch.no_grad():
            outputs = model(inputs)

        print("\nOutput feature shapes:")
        for i, out in enumerate(outputs):
            print(f"  Stage {i}: {out.shape}") # Should be [B, C_i, H_i, W_i]

        print(f"\nCalculated width list (channels): {model.width_list}")
        # Verify width list matches output channels
        assert model.width_list == [o.shape[1] for o in outputs], "Width list mismatch!"
        print("Width list check passed.")

        # --- FLOPs and Parameter Count ---
        param = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"\nNumber of Parameters: {param / 1e6:.2f} M")

        try:
            from fvcore.nn import FlopCountAnalysis, flop_count_table # Removed ActivationCountAnalysis for simplicity

            # Use B=1 for standard FLOP count reporting
            inputs_flop = torch.randn(1, 3, 224, 224).to(device)
            flops = FlopCountAnalysis(model, inputs_flop)

            print(f"\n--- FLOPs Analysis (fvcore) ---")
            print(flop_count_table(flops, max_depth=3)) # Show top-level FLOPs breakdown
            print(f"Total GFLOPs: {flops.total() / 1e9:.2f} G")

        except ImportError:
            print("\nInstall fvcore (pip install fvcore) for detailed FLOPs analysis.")
        except Exception as e:
            print(f"\nError during FLOPs analysis: {e}")

    except Exception as e:
        print("\n--- ERROR DURING FORWARD PASS ---")
        import traceback
        traceback.print_exc()